{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COST INTERACT ML Challenge (NET) - Baseline\n",
    "\n",
    "The code contained in this notebook serves as the baseline code provided by the organizers of the COST INTERACT ML challenge (NET). The baseline implements a shallow neural network with gaussian output as a probabilistic regressor. The baseline is provided as a starting point for participants to build upon.\n",
    "\n",
    "Authors: Marco Skocaj (HA1 Chair, Università di Bologna, Italy), Nicola Di Cicco (Politecnico di Milano, Italy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # install tf with pip install tensorflow==2.10\n",
    "import tensorflow_probability as tfp # install tfp with pip install tensorflow-probability==0.18\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check tensorflow version (tf <= 2.10 required for native gpu support on Windows)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Check tf is running on gpu\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:25.323718200Z",
     "start_time": "2023-08-25T13:49:16.164614700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Your local starting kit path here\n",
    "data_fpath = r\"C:\\Users\\skoca\\PycharmProjects\\starting_kit_NET\"\n",
    "fpath_train = f\"{data_fpath}/train.csv\"\n",
    "fpath_val = f\"{data_fpath}/val_no_labels.csv\"\n",
    "fpath_test = f\"{data_fpath}/test_no_labels.csv\"\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(fpath_train)\n",
    "val = pd.read_csv(fpath_val)\n",
    "test = pd.read_csv(fpath_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:25.371762400Z",
     "start_time": "2023-08-25T13:49:25.325221800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:26.713972Z",
     "start_time": "2023-08-25T13:49:25.363848800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                128       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " distribution_lambda (Distri  ((None, 1),              0         \n",
      " butionLambda)                (None, 1))                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 194\n",
      "Trainable params: 194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a probabilistic model. In this example we use a shallow probabilistic neural network with a gaussian output\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tfp.layers.DistributionLambda(\n",
    "        lambda t: tfp.distributions.Normal(loc=t[..., :1],\n",
    "                                           scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]))),\n",
    "])\n",
    "\n",
    "# Define loss function (negative log likelihood)\n",
    "def nll(y_true, dist):\n",
    "    return -dist.log_prob(y_true)\n",
    "\n",
    "# Build model\n",
    "model.build(input_shape=(None, 3))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=nll)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:39.573468700Z",
     "start_time": "2023-08-25T13:49:26.713972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 2s 3ms/step - loss: 0.8248\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.9557\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.4673\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3654\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.1487\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2649\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.0749\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.1732\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.3910\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: -0.6364\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.8460\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: -0.4568\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.6287\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.9454\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: -1.1847\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -0.5110\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -1.0804\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 0s 2ms/step - loss: -1.2688\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: -1.3318\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 0s 3ms/step - loss: -0.3715\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.81it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        point     lo_80     hi_80     lo_90     hi_90     lo_95     hi_95\n0    0.085019  0.019817  0.140848 -0.018095  0.162632 -0.023904  0.210095\n1    0.095311  0.013366  0.186220  0.008445  0.203451  0.003010  0.216515\n2    0.084715 -0.086970  0.260772 -0.130551  0.289418 -0.162904  0.323550\n3    0.094406  0.018040  0.165642 -0.009483  0.176935 -0.015448  0.188620\n4    0.104588 -0.011762  0.197887 -0.033109  0.201371 -0.054417  0.203565\n..        ...       ...       ...       ...       ...       ...       ...\n450 -5.402172 -6.665071 -4.116567 -7.272796 -3.656495 -7.445249 -3.600164\n451 -2.667571 -3.497131 -1.926564 -3.779995 -1.536213 -4.022359 -1.403440\n452 -1.910905 -2.739428 -1.155323 -3.030740 -1.085644 -3.177084 -1.057540\n453 -5.897260 -7.403997 -4.548115 -8.211551 -4.222713 -8.537130 -4.183210\n454 -5.548982 -7.066078 -3.979962 -7.778115 -3.422186 -8.130587 -3.352283\n\n[455 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>point</th>\n      <th>lo_80</th>\n      <th>hi_80</th>\n      <th>lo_90</th>\n      <th>hi_90</th>\n      <th>lo_95</th>\n      <th>hi_95</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.085019</td>\n      <td>0.019817</td>\n      <td>0.140848</td>\n      <td>-0.018095</td>\n      <td>0.162632</td>\n      <td>-0.023904</td>\n      <td>0.210095</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.095311</td>\n      <td>0.013366</td>\n      <td>0.186220</td>\n      <td>0.008445</td>\n      <td>0.203451</td>\n      <td>0.003010</td>\n      <td>0.216515</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.084715</td>\n      <td>-0.086970</td>\n      <td>0.260772</td>\n      <td>-0.130551</td>\n      <td>0.289418</td>\n      <td>-0.162904</td>\n      <td>0.323550</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.094406</td>\n      <td>0.018040</td>\n      <td>0.165642</td>\n      <td>-0.009483</td>\n      <td>0.176935</td>\n      <td>-0.015448</td>\n      <td>0.188620</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104588</td>\n      <td>-0.011762</td>\n      <td>0.197887</td>\n      <td>-0.033109</td>\n      <td>0.201371</td>\n      <td>-0.054417</td>\n      <td>0.203565</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>450</th>\n      <td>-5.402172</td>\n      <td>-6.665071</td>\n      <td>-4.116567</td>\n      <td>-7.272796</td>\n      <td>-3.656495</td>\n      <td>-7.445249</td>\n      <td>-3.600164</td>\n    </tr>\n    <tr>\n      <th>451</th>\n      <td>-2.667571</td>\n      <td>-3.497131</td>\n      <td>-1.926564</td>\n      <td>-3.779995</td>\n      <td>-1.536213</td>\n      <td>-4.022359</td>\n      <td>-1.403440</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>-1.910905</td>\n      <td>-2.739428</td>\n      <td>-1.155323</td>\n      <td>-3.030740</td>\n      <td>-1.085644</td>\n      <td>-3.177084</td>\n      <td>-1.057540</td>\n    </tr>\n    <tr>\n      <th>453</th>\n      <td>-5.897260</td>\n      <td>-7.403997</td>\n      <td>-4.548115</td>\n      <td>-8.211551</td>\n      <td>-4.222713</td>\n      <td>-8.537130</td>\n      <td>-4.183210</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>-5.548982</td>\n      <td>-7.066078</td>\n      <td>-3.979962</td>\n      <td>-7.778115</td>\n      <td>-3.422186</td>\n      <td>-8.130587</td>\n      <td>-3.352283</td>\n    </tr>\n  </tbody>\n</table>\n<p>455 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Training model...\")\n",
    "model.fit(train.drop('UL Throughput (Mbps)', axis=1), train['UL Throughput (Mbps)'], epochs=20, verbose=1)\n",
    "\n",
    "### Compute metrics ###\n",
    "# Perform an MC experiment to get the predictive distribution\n",
    "n_experiments = 50\n",
    "print(\"Predicting...\")\n",
    "preds_val = np.stack([model.predict(val, verbose=0) for _ in tqdm(range(n_experiments))]).squeeze()\n",
    "preds_test = np.stack([model.predict(test, verbose=0) for _ in tqdm(range(n_experiments))]).squeeze()\n",
    "\n",
    "# Compute a point estimate of your predictive distribution. In this case, the mean of the Gaussian\n",
    "preds_val_mean = np.mean(preds_val, axis=0)\n",
    "\n",
    "# Compute the 80% CI of the predictive distribution, low and high bounds\n",
    "preds_val_low_80 = np.quantile(preds_val, 0.1, axis=0)\n",
    "preds_val_high_80 = np.quantile(preds_val, 0.9, axis=0)\n",
    "\n",
    "# Compute the 90% CI of the predictive distribution, low and high bounds\n",
    "preds_val_low_90 = np.quantile(preds_val, 0.05, axis=0)\n",
    "preds_val_high_90 = np.quantile(preds_val, 0.95, axis=0)\n",
    "\n",
    "# Compute the 95% CI of the predictive distribution, low and high bounds\n",
    "preds_val_low_95 = np.quantile(preds_val, 0.025, axis=0)\n",
    "preds_val_high_95 = np.quantile(preds_val, 0.975, axis=0)\n",
    "\n",
    "# Create a dataframe with the predictions metrics\n",
    "preds_val_df = pd.DataFrame({'point': preds_val_mean,\n",
    "                             'lo_80': preds_val_low_80,\n",
    "                             'hi_80': preds_val_high_80,\n",
    "                             'lo_90': preds_val_low_90,\n",
    "                             'hi_90': preds_val_high_90,\n",
    "                             'lo_95': preds_val_low_95,\n",
    "                             'hi_95': preds_val_high_95})\n",
    "\n",
    "preds_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:40.883792900Z",
     "start_time": "2023-08-25T13:49:40.781026700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to preds.zip\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to csv file and zip it.\n",
    "#  The zipped file should be used for score submission on Codalab.\n",
    "preds_val_df.to_csv(f\"{data_fpath}/preds.csv\", index=False)\n",
    "is_written = os.system(f\"zip preds.zip preds.csv\")\n",
    "\n",
    "if ~is_written:\n",
    "    print(\"Predictions saved to preds.zip\")\n",
    "else:\n",
    "    print(\"Error while saving predictions to preds.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:43.486870400Z",
     "start_time": "2023-08-25T13:49:43.462886200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.11347505576042179\n",
      "Coverage (80% CI, 90% CI, 95% CI):  [0.07142857142857142, 0.03351648351648354, 0.013186813186813197]\n",
      "Sharpness (80% CI, 90% CI, 95% CI):  [0.42768791651496524, 0.5426242482633543, 0.6266905651593598]\n"
     ]
    }
   ],
   "source": [
    "# You can test the logic of the scoring mechanism on Codalab here.\n",
    "#  Just be sure to keep a held-out validation test from the original training set with features and labels.\n",
    "\n",
    "def score(ground_truth: pd.DataFrame, preds: pd.DataFrame):\n",
    "    # Check that the submission file has the correct headers\n",
    "    expected_headers = [\"point\", \"lo_80\", \"hi_80\", \"lo_90\", \"hi_90\", \"lo_95\", \"hi_95\"]\n",
    "    preds_headers = list(preds.columns)\n",
    "    if(set(expected_headers) != set(preds_headers)):\n",
    "        print(f\"Submission file headers do not match expected headers. Please double-check the submission file.\\nExpected headers: {expected_headers}\\nSubmission file headers: {preds_headers}\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    ci_levels = [80, 90, 95]\n",
    "    ul_thp = ground_truth[\"UL Throughput (Mbps)\"]\n",
    "    calibration_errors = []\n",
    "    sharpnesses = []\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = np.mean(np.abs(ul_thp - preds[\"point\"]))\n",
    "    if(np.isnan(mae)):\n",
    "        print(f\"MAE is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    # Compute coverage and sharpness for each confidence level\n",
    "    for level in ci_levels:\n",
    "        hi_level = preds[f\"hi_{level}\"]\n",
    "        lo_level = preds[f\"lo_{level}\"]\n",
    "\n",
    "        # Compute the coverage\n",
    "        # Calculate the quantiles for the centered confidence intervals\n",
    "        offset = (100 - level) / 2\n",
    "        quantile_lo = offset / 100\n",
    "        quantile_hi = 1 - offset / 100\n",
    "        # sanity check\n",
    "        assert np.isclose(quantile_hi - quantile_lo, level / 100)\n",
    "\n",
    "        # Calculate the empirical coverage for both quantiles\n",
    "        coverage_lo = np.mean(ul_thp <= lo_level)\n",
    "        coverage_hi = np.mean(ul_thp <= hi_level)\n",
    "\n",
    "        # Calculate the calibration errors\n",
    "        calibration_error_lo = np.abs(coverage_lo - quantile_lo)\n",
    "        calibration_error_hi = np.abs(coverage_hi - quantile_hi)\n",
    "        calibration_errors.append((calibration_error_lo + calibration_error_hi) / 2)\n",
    "\n",
    "        # Compute the sharpness\n",
    "        sharpness = np.mean(hi_level - lo_level)\n",
    "        sharpnesses.append(sharpness)\n",
    "        if(np.isnan(sharpness)):\n",
    "            print(f\"Sharpness at {level}% is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "            return 404, 404, 404\n",
    "    \n",
    "    \n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"Coverage (80% CI, 90% CI, 95% CI): \", calibration_errors)\n",
    "    print(\"Sharpness (80% CI, 90% CI, 95% CI): \", sharpnesses)\n",
    "\n",
    "truth = pd.read_csv(r\"C:\\Users\\skoca\\PycharmProjects\\MLcomp_INTERACT_NET_bundle\\reference_data_val\\val_ref.csv\")\n",
    "pred = pd.read_csv(f\"{data_fpath}/preds.csv\")\n",
    "\n",
    "score(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
