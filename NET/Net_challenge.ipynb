{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COST INTERACT ML Challenge (NET) - Baseline\n",
    "\n",
    "The code contained in this notebook serves as the baseline code provided by the organizers of the COST INTERACT ML challenge (NET). The baseline implements a shallow neural network with gaussian output as a probabilistic regressor. The baseline is provided as a starting point for participants to build upon.\n",
    "\n",
    "Authors: Marco Skocaj (HA1 Chair, Universit√† di Bologna, Italy), Nicola Di Cicco (Politecnico di Milano, Italy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:25.323718200Z",
     "start_time": "2023-08-25T13:49:16.164614700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 13:06:38.551571: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-02 13:06:39.164958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # install tf with pip install tensorflow==2.10\n",
    "import tensorflow_probability as tfp # install tfp with pip install tensorflow-probability==0.18\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Check tensorflow version (tf <= 2.10 required for native gpu support on Windows)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Check tf is running on gpu\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:25.371762400Z",
     "start_time": "2023-08-25T13:49:25.325221800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your local starting kit path here\n",
    "data_fpath = r\".\"\n",
    "fpath_train = f\"{data_fpath}/train.csv\"\n",
    "fpath_val = f\"{data_fpath}/val_no_labels.csv\"\n",
    "fpath_test = f\"{data_fpath}/test_no_labels.csv\"\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(fpath_train)\n",
    "val = pd.read_csv(fpath_val)\n",
    "test = pd.read_csv(fpath_test)\n",
    "\n",
    "# train = train.drop((train[train['DL Throughput (Mbps)'] < -3].index))\n",
    "\n",
    "X = train.drop('UL Throughput (Mbps)', axis=1).values.astype(np.float32)\n",
    "y = train['UL Throughput (Mbps)'].values.astype(np.float32).reshape(-1, 1)\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X, y, test_size=0.05, random_state=0)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=23000)\n",
    "dataset_train = dataset_train.batch(128)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, dist):\n",
    "    return -dist.log_prob(y_true)\n",
    "\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense0 = tf.keras.layers.Dense(units=8, activation=tf.nn.relu)\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)\n",
    "        # self.dense2 = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)\n",
    "        # self.dense3 = tf.keras.layers.Dense(units=512, activation=tf.nn.relu)\n",
    "        # self.dense4 = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)\n",
    "        self.dense6 = tf.keras.layers.Dense(units=2)\n",
    "        self.dist = tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:])))\n",
    "        # self.dist = tfp.layers.DistributionLambda(lambda t: tfp.distributions.Mixture(cat=tfp.distributions.Categorical(probs=[0.9, 0.1]), components=[tfp.distributions.Normal(loc=t[..., 0:1], scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:2])), tfp.distributions.Normal(loc=t[..., 2:3], scale=1e-3 + tf.math.softplus(0.05 * t[..., 3:4]))]))\n",
    "         # self.dist = tfp.layers.DistributionLambda(lambda t: tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=[0.9, 0.1]), components_distribution=tfd.Normal(loc=[t[..., 0:1], t[..., 1:2]], scale=[1e-3 + tf.math.softplus(0.05 * t[..., 1:2]), 1e-3 + tf.math.softplus(0.05 * t[..., 1:2])]))    \n",
    "    def call(self, x):\n",
    "        x = self.dense0(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.dense2(x)\n",
    "        # x = self.dense3(x)\n",
    "        # x = self.dense4(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dense6(x)\n",
    "        output = self.dist(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "scheduler1 = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=100, decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 13:11:58.902760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bdf98bbec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-02 13:11:58.902781: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-02 13:11:58.918810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-02 13:11:59.062653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-02 13:11:59.213535: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fa8efbbb040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fa8efbbb040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "round: 0 loss:  0.6163719\n",
      "round: 1 loss:  0.5339272\n",
      "round: 2 loss:  0.6289394\n",
      "round: 3 loss:  0.50380397\n",
      "round: 4 loss:  0.64707404\n",
      "round: 5 loss:  0.28324983\n",
      "round: 6 loss:  -0.3141638\n",
      "round: 7 loss:  -0.17199703\n",
      "round: 8 loss:  -0.9404268\n",
      "round: 9 loss:  -0.3888808\n",
      "round: 10 loss:  -0.48421755\n",
      "round: 11 loss:  -1.0106201\n",
      "round: 12 loss:  1.5479771\n",
      "round: 13 loss:  -0.076039694\n",
      "round: 14 loss:  -0.048417766\n",
      "round: 15 loss:  -0.21204178\n",
      "round: 16 loss:  -0.46313727\n",
      "round: 17 loss:  -0.73914564\n",
      "round: 18 loss:  -1.0029184\n",
      "round: 19 loss:  -0.8970388\n",
      "round: 20 loss:  -1.044597\n",
      "round: 21 loss:  -1.4470835\n",
      "round: 22 loss:  -1.6130128\n",
      "round: 23 loss:  -0.98073334\n",
      "round: 24 loss:  -1.0237486\n",
      "round: 25 loss:  -1.0025254\n",
      "round: 26 loss:  -1.2352535\n",
      "round: 27 loss:  -1.3033665\n",
      "round: 28 loss:  -1.8012013\n",
      "round: 29 loss:  -1.4212484\n",
      "round: 30 loss:  -1.725923\n",
      "round: 31 loss:  -1.4909743\n",
      "round: 32 loss:  -1.4005998\n",
      "round: 33 loss:  -1.7261294\n",
      "round: 34 loss:  -1.5165104\n",
      "round: 35 loss:  -1.4942205\n",
      "round: 36 loss:  -1.4477645\n",
      "round: 37 loss:  -1.8824888\n",
      "round: 38 loss:  -1.613254\n",
      "round: 39 loss:  -1.3651382\n",
      "round: 40 loss:  -1.2703385\n",
      "round: 41 loss:  -1.2839837\n",
      "round: 42 loss:  -1.6684119\n",
      "round: 43 loss:  -1.6738833\n",
      "round: 44 loss:  -1.497052\n",
      "round: 45 loss:  -1.6954103\n",
      "round: 46 loss:  -1.4305087\n",
      "round: 47 loss:  -1.4725243\n",
      "round: 48 loss:  -1.9478543\n",
      "round: 49 loss:  -1.7021669\n",
      "round: 50 loss:  -1.478527\n",
      "round: 51 loss:  -1.8578798\n",
      "round: 52 loss:  -1.4713624\n",
      "round: 53 loss:  -1.3536953\n",
      "round: 54 loss:  -1.4177022\n",
      "round: 55 loss:  -1.7290523\n",
      "round: 56 loss:  -1.0004045\n",
      "round: 57 loss:  -1.6319492\n",
      "round: 58 loss:  -1.7918265\n",
      "round: 59 loss:  -1.8793249\n",
      "round: 60 loss:  -1.2746125\n",
      "round: 61 loss:  -1.3509895\n",
      "round: 62 loss:  -1.3293642\n",
      "round: 63 loss:  -1.6249583\n",
      "round: 64 loss:  -1.7353084\n",
      "round: 65 loss:  -1.560581\n",
      "round: 66 loss:  -1.5924879\n",
      "round: 67 loss:  -1.8475147\n",
      "round: 68 loss:  -1.3526604\n",
      "round: 69 loss:  -1.4270945\n",
      "round: 70 loss:  -1.6009955\n",
      "round: 71 loss:  -1.8089932\n",
      "round: 72 loss:  -1.5339621\n",
      "round: 73 loss:  -1.5163301\n",
      "round: 74 loss:  -1.5507652\n",
      "round: 75 loss:  -1.5279304\n",
      "round: 76 loss:  -1.5381978\n",
      "round: 77 loss:  -1.5841208\n",
      "round: 78 loss:  -1.4797825\n",
      "round: 79 loss:  -1.5267495\n",
      "round: 80 loss:  -1.5332475\n",
      "round: 81 loss:  -1.2455682\n",
      "round: 82 loss:  -1.4918225\n",
      "round: 83 loss:  -1.6210788\n",
      "round: 84 loss:  -1.762757\n",
      "round: 85 loss:  -1.8501335\n",
      "round: 86 loss:  -1.4096581\n",
      "round: 87 loss:  -1.8964071\n",
      "round: 88 loss:  -1.436049\n",
      "round: 89 loss:  -1.778234\n",
      "round: 90 loss:  -1.9936434\n",
      "round: 91 loss:  -1.9774209\n",
      "round: 92 loss:  -1.4116302\n",
      "round: 93 loss:  -1.7868963\n",
      "round: 94 loss:  -1.9159105\n",
      "round: 95 loss:  -1.8284543\n",
      "round: 96 loss:  -1.7938979\n",
      "round: 97 loss:  -1.3592347\n",
      "round: 98 loss:  -1.6436076\n",
      "round: 99 loss:  -1.5762393\n",
      "round: 100 loss:  -1.1792135\n",
      "round: 101 loss:  -1.7086831\n",
      "round: 102 loss:  -1.6013541\n",
      "round: 103 loss:  -1.5309882\n",
      "round: 104 loss:  -1.5147922\n",
      "round: 105 loss:  -1.8776164\n",
      "round: 106 loss:  -1.9014381\n",
      "round: 107 loss:  -1.6703717\n",
      "round: 108 loss:  -1.6090221\n",
      "round: 109 loss:  -1.8645563\n",
      "round: 110 loss:  -1.8954014\n",
      "round: 111 loss:  -1.7756733\n",
      "round: 112 loss:  -1.9446541\n",
      "round: 113 loss:  -1.8406956\n",
      "round: 114 loss:  -1.7040967\n",
      "round: 115 loss:  -1.7118378\n",
      "round: 116 loss:  -1.8100076\n",
      "round: 117 loss:  -1.8862649\n",
      "round: 118 loss:  -1.8112466\n",
      "round: 119 loss:  -1.7241809\n",
      "round: 120 loss:  -1.8059621\n",
      "round: 121 loss:  -1.9047372\n",
      "round: 122 loss:  -1.2041314\n",
      "round: 123 loss:  -1.7721554\n",
      "round: 124 loss:  -1.3926257\n",
      "round: 125 loss:  -1.6517376\n",
      "round: 126 loss:  -1.6320225\n",
      "round: 127 loss:  -1.6020204\n",
      "round: 128 loss:  -2.1774662\n",
      "round: 129 loss:  -1.6367794\n",
      "round: 130 loss:  -1.8845735\n",
      "round: 131 loss:  -1.7653127\n",
      "round: 132 loss:  -1.9767195\n",
      "round: 133 loss:  -1.3259048\n",
      "round: 134 loss:  -1.6879787\n",
      "round: 135 loss:  -1.6619343\n",
      "round: 136 loss:  -1.4795998\n",
      "round: 137 loss:  -1.7984309\n",
      "round: 138 loss:  -1.756599\n",
      "round: 139 loss:  -1.656308\n",
      "round: 140 loss:  -1.8655714\n",
      "round: 141 loss:  -1.4553711\n",
      "round: 142 loss:  -1.8753344\n",
      "round: 143 loss:  -1.8222365\n",
      "round: 144 loss:  -1.4194287\n",
      "round: 145 loss:  -0.9615498\n",
      "round: 146 loss:  -1.8870792\n",
      "round: 147 loss:  -1.8683583\n",
      "round: 148 loss:  -1.7932135\n",
      "round: 149 loss:  -1.8799304\n",
      "round: 150 loss:  -1.6440313\n",
      "round: 151 loss:  -1.6161927\n",
      "round: 152 loss:  -1.8544711\n",
      "round: 153 loss:  -1.8160117\n",
      "round: 154 loss:  -1.8625355\n",
      "round: 155 loss:  -1.380226\n",
      "round: 156 loss:  -1.640806\n",
      "round: 157 loss:  -1.7956661\n",
      "round: 158 loss:  -1.6609179\n",
      "round: 159 loss:  -1.6640642\n",
      "round: 160 loss:  -1.9614542\n",
      "round: 161 loss:  -1.7956619\n",
      "round: 162 loss:  -1.7078174\n",
      "round: 163 loss:  -1.8233656\n",
      "round: 164 loss:  -2.0425642\n",
      "round: 165 loss:  -1.7913907\n",
      "round: 166 loss:  -1.2615242\n",
      "round: 167 loss:  -1.3918822\n",
      "round: 168 loss:  -1.7419096\n",
      "round: 169 loss:  -1.4156296\n",
      "round: 170 loss:  -1.8013215\n",
      "round: 171 loss:  -1.758493\n",
      "round: 172 loss:  -1.7822872\n",
      "round: 173 loss:  -1.7347219\n",
      "round: 174 loss:  -1.7194455\n",
      "round: 175 loss:  -1.9948419\n",
      "round: 176 loss:  -1.7418957\n",
      "round: 177 loss:  -1.73097\n",
      "round: 178 loss:  -1.7692552\n",
      "round: 179 loss:  -1.4785582\n",
      "round: 180 loss:  -2.149985\n",
      "round: 181 loss:  -1.6699902\n",
      "round: 182 loss:  -2.0416155\n",
      "round: 183 loss:  -1.5747675\n",
      "round: 184 loss:  -1.622564\n",
      "round: 185 loss:  -1.7162035\n",
      "round: 186 loss:  -1.5549897\n",
      "round: 187 loss:  -1.9755349\n",
      "round: 188 loss:  -1.5768569\n",
      "round: 189 loss:  -1.5339274\n",
      "round: 190 loss:  -1.5726529\n",
      "round: 191 loss:  -1.944231\n",
      "round: 192 loss:  -1.8016356\n",
      "round: 193 loss:  -1.9411104\n",
      "round: 194 loss:  -1.7118715\n",
      "round: 195 loss:  -1.6081306\n",
      "round: 196 loss:  -1.7556709\n",
      "round: 197 loss:  -1.8463751\n",
      "round: 198 loss:  -1.7809291\n",
      "round: 199 loss:  -1.8844227\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    for X, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(X)\n",
    "            loss = tf.reduce_mean(nll(y, pred))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    print(\"round:\", i, \"loss: \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_metrics(val):\n",
    "    ### Compute metrics ###\n",
    "\n",
    "    preds_val = model(val)\n",
    "    \n",
    "    # Compute a point estimate of your predictive distribution. In this case, the mean of the Gaussian\n",
    "    preds_val_mean = preds_val.mean().numpy().reshape(-1)\n",
    "    \n",
    "    # Compute the 80% CI of the predictive distribution, low and high bounds\n",
    "    preds_val_low_80 =  preds_val.quantile(0.1).numpy().reshape(-1)\n",
    "    preds_val_high_80 = preds_val.quantile(0.9).numpy().reshape(-1)\n",
    "    \n",
    "    # Compute the 90% CI of the predictive distribution, low and high bounds\n",
    "    preds_val_low_90 = preds_val.quantile(0.05).numpy().reshape(-1)\n",
    "    preds_val_high_90 = preds_val.quantile(0.95).numpy().reshape(-1)\n",
    "    \n",
    "    # Compute the 95% CI of the predictive distribution, low and high bounds\n",
    "    preds_val_low_95 = preds_val.quantile(0.025).numpy().reshape(-1)\n",
    "    preds_val_high_95 = preds_val.quantile(0.975).numpy().reshape(-1)\n",
    "    \n",
    "    preds_val_df = pd.DataFrame({'point': preds_val_mean,\n",
    "                                 'lo_80': preds_val_low_80,\n",
    "                                 'hi_80': preds_val_high_80,\n",
    "                                 'lo_90': preds_val_low_90,\n",
    "                                 'hi_90': preds_val_high_90,\n",
    "                                 'lo_95': preds_val_low_95,\n",
    "                                 'hi_95': preds_val_high_95})\n",
    "\n",
    "    return preds_val_df\n",
    "\n",
    "\n",
    "def score(ground_truth: pd.DataFrame, preds: pd.DataFrame):\n",
    "    # Check that the submission file has the correct headers\n",
    "    expected_headers = [\"point\", \"lo_80\", \"hi_80\", \"lo_90\", \"hi_90\", \"lo_95\", \"hi_95\"]\n",
    "    preds_headers = list(preds.columns)\n",
    "    if(set(expected_headers) != set(preds_headers)):\n",
    "        print(f\"Submission file headers do not match expected headers. Please double-check the submission file.\\nExpected headers: {expected_headers}\\nSubmission file headers: {preds_headers}\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    ci_levels = [80, 90, 95]\n",
    "    ul_thp = ground_truth[\"UL Throughput (Mbps)\"]\n",
    "    calibration_errors = []\n",
    "    sharpnesses = []\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = np.mean(np.abs(ul_thp - preds[\"point\"]))\n",
    "    if(np.isnan(mae)):\n",
    "        print(f\"MAE is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    # Compute coverage and sharpness for each confidence level\n",
    "    for level in ci_levels:\n",
    "        hi_level = preds[f\"hi_{level}\"]\n",
    "        lo_level = preds[f\"lo_{level}\"]\n",
    "\n",
    "        # Compute the coverage\n",
    "        # Calculate the quantiles for the centered confidence intervals\n",
    "        offset = (100 - level) / 2\n",
    "        quantile_lo = offset / 100\n",
    "        quantile_hi = 1 - offset / 100\n",
    "        # sanity check\n",
    "        assert np.isclose(quantile_hi - quantile_lo, level / 100)\n",
    "\n",
    "        # Calculate the empirical coverage for both quantiles\n",
    "        coverage_lo = np.mean(ul_thp <= lo_level)\n",
    "        coverage_hi = np.mean(ul_thp <= hi_level)\n",
    "\n",
    "        # Calculate the calibration errors\n",
    "        calibration_error_lo = np.abs(coverage_lo - quantile_lo)\n",
    "        calibration_error_hi = np.abs(coverage_hi - quantile_hi)\n",
    "        calibration_errors.append((calibration_error_lo + calibration_error_hi) / 2)\n",
    "\n",
    "        # Compute the sharpness\n",
    "        sharpness = np.mean(hi_level - lo_level)\n",
    "        sharpnesses.append(sharpness)\n",
    "        if(np.isnan(sharpness)):\n",
    "            print(f\"Sharpness at {level}% is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "            return 404, 404, 404\n",
    "    \n",
    "    \n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"Coverage (80% CI, 90% CI, 95% CI): \", calibration_errors)\n",
    "    print(\"Sharpness (80% CI, 90% CI, 95% CI): \", sharpnesses)\n",
    "    print(\"Total: \", mae + np.sum(calibration_errors) + 0.25*np.sum(sharpnesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.029437993\n",
      "Coverage (80% CI, 90% CI, 95% CI):  [0.06261682242990654, 0.026635514018691613, 0.018691588785046738]\n",
      "Sharpness (80% CI, 90% CI, 95% CI):  [0.11866239, 0.15230154, 0.18147849]\n",
      "Total:  0.250492519857449\n"
     ]
    }
   ],
   "source": [
    "prednew = predict_metrics(X_vali)\n",
    "truthnew = pd.DataFrame({'UL Throughput (Mbps)': y_vali.reshape(-1)})\n",
    "score(truthnew, prednew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models_1/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, './models_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('./models_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: test.csv (deflated 57%)\n",
      "Predictions saved to preds.zip\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to csv file and zip it.\n",
    "#  The zipped file should be used for score submission on Codalab.\n",
    "\n",
    "preds_val_df = predict_metrics(test.values)\n",
    "\n",
    "preds_val_df.to_csv(f\"{data_fpath}/test.csv\", index=False)\n",
    "is_written = os.system(f\"zip test.zip test.csv\")\n",
    "\n",
    "if ~is_written:\n",
    "    print(\"Predictions saved to preds.zip\")\n",
    "else:\n",
    "    print(\"Error while saving predictions to preds.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other attempts, However, none of them is working better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change NN network\n",
    "def nll(y_true, dist):\n",
    "    return -dist.log_prob(y_true)\n",
    "\n",
    "\n",
    "\n",
    "class Reshape1(tf.keras.Model):\n",
    "    def call(self, x):\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reshape = Reshape1()\n",
    "        self.Con1 = tf.keras.layers.Conv2D(filters=3, kernel_size=3, activation='relu', padding='same')\n",
    "        self.Con2 = tf.keras.layers.Conv2D(filters=2, kernel_size=2, activation='relu', padding='same')\n",
    "        self.Con3 = tf.keras.layers.Flatten()\n",
    "        self.Con4 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.Con5 = tf.keras.layers.Dense(16, activation='relu')\n",
    "        self.dense1 = tf.keras.layers.Dense(2)\n",
    "        self.dist = tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:])))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.reshape(x)\n",
    "        print(x.shape)\n",
    "        x = self.Con1(x)\n",
    "        x = self.Con2(x)\n",
    "        x = self.Con3(x)\n",
    "        x = self.Con4(x)\n",
    "        x = self.Con5(x)\n",
    "        # x = self.Con6(x)\n",
    "        # x = self.Con7(x)\n",
    "        x = self.dense1(x)\n",
    "        output = self.dist(x)\n",
    "        return output\n",
    "\n",
    "model = CNN()\n",
    "scheduler1 = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.003, decay_steps=100, decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler1)\n",
    "\n",
    "\n",
    "# X = tf.random.uniform(shape=(100, 3))\n",
    "# for layer in net().layers:\n",
    "#     X = layer(X)\n",
    "#     print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T13:49:43.486870400Z",
     "start_time": "2023-08-25T13:49:43.462886200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change loss function\n",
    "\n",
    "# You can test the logic of the scoring mechanism on Codalab here.\n",
    "#  Just be sure to keep a held-out validation test from the original training set with features and labels.\n",
    "\n",
    "def score(ground_truth: pd.DataFrame, preds: pd.DataFrame):\n",
    "    print(ground_truth)\n",
    "    print(preds)\n",
    "    # Check that the submission file has the correct headers\n",
    "    expected_headers = [\"point\", \"lo_80\", \"hi_80\", \"lo_90\", \"hi_90\", \"lo_95\", \"hi_95\"]\n",
    "    preds_headers = list(preds.columns)\n",
    "    if(set(expected_headers) != set(preds_headers)):\n",
    "        print(f\"Submission file headers do not match expected headers. Please double-check the submission file.\\nExpected headers: {expected_headers}\\nSubmission file headers: {preds_headers}\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    ci_levels = [80, 90, 95]\n",
    "    ul_thp = ground_truth[\"UL Throughput (Mbps)\"]\n",
    "    calibration_errors = []\n",
    "    sharpnesses = []\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = np.mean(np.abs(ul_thp - preds[\"point\"]))\n",
    "    if(np.isnan(mae)):\n",
    "        print(f\"MAE is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "        return 404, 404, 404\n",
    "\n",
    "    # Compute coverage and sharpness for each confidence level\n",
    "    for level in ci_levels:\n",
    "        hi_level = preds[f\"hi_{level}\"]\n",
    "        lo_level = preds[f\"lo_{level}\"]\n",
    "\n",
    "        # Compute the coverage\n",
    "        # Calculate the quantiles for the centered confidence intervals\n",
    "        offset = (100 - level) / 2\n",
    "        quantile_lo = offset / 100\n",
    "        quantile_hi = 1 - offset / 100\n",
    "        # sanity check\n",
    "        assert np.isclose(quantile_hi - quantile_lo, level / 100)\n",
    "\n",
    "        # Calculate the empirical coverage for both quantiles\n",
    "        coverage_lo = np.mean(ul_thp <= lo_level)\n",
    "        coverage_hi = np.mean(ul_thp <= hi_level)\n",
    "\n",
    "        # Calculate the calibration errors\n",
    "        calibration_error_lo = np.abs(coverage_lo - quantile_lo)\n",
    "        calibration_error_hi = np.abs(coverage_hi - quantile_hi)\n",
    "        calibration_errors.append((calibration_error_lo + calibration_error_hi) / 2)\n",
    "\n",
    "        # Compute the sharpness\n",
    "        sharpness = np.mean(hi_level - lo_level)\n",
    "        sharpnesses.append(sharpness)\n",
    "        if(np.isnan(sharpness)):\n",
    "            print(f\"Sharpness at {level}% is NaN, submission file invalid. Please double-check the submission file.\")\n",
    "            return 404, 404, 404\n",
    "    \n",
    "    \n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"Coverage (80% CI, 90% CI, 95% CI): \", calibration_errors)\n",
    "    print(\"Sharpness (80% CI, 90% CI, 95% CI): \", sharpnesses)\n",
    "\n",
    "# truth = pd.read_csv(r\"C:\\Users\\skoca\\PycharmProjects\\MLcomp_INTERACT_NET_bundle\\reference_data_val\\val_ref.csv\")\n",
    "# pred = pd.read_csv(f\"{data_fpath}/preds.csv\")\n",
    "\n",
    "# score(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change the distribution function at the output layer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tfd = tfp.distributions\n",
    "x = np.linspace(-2., 3., int(1e4), dtype=np.float32)\n",
    "gm = tfd.MixtureSameFamily(\n",
    "  mixture_distribution=tfd.Categorical(\n",
    "      probs=[0.9, 0.1]),\n",
    "  components_distribution=tfd.Normal(\n",
    "    loc=[-1., 2],       # One for each component.\n",
    "    scale=[0.3, 0.1]))  # And same here.\n",
    "plt.plot(x, gm.prob(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing dataset\n",
    "\n",
    "#Dat train[train['UL Throughput (Mbps)'] < -1] = -1 print(sum(train['UL Throughput (Mbps)'] == -1)) train['UL Throughput (Mbps)'].hist()\n",
    "\n",
    "(-(+train['UL Throughput (Mbps)'])+1).apply(np.log2).hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
